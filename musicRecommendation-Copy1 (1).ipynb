{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext.stop(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '100g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory','300g')\n",
    "sc = SparkContext('local',\"recommendation\")\n",
    "sql_sc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import spark package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf().setAppName(\"App\")\n",
    "conf = (conf.setMaster('local[*]').set('spark.executor.memory', '140G').set('spark.driver.memory', '450G').set('spark.driver.maxResultSize', '140G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '10g')\n",
    "#SparkContext.setSystemProperty('spark.driver.memory','30g')\n",
    "#sc = SparkContext('local',appName=\"recommendation\")\n",
    "sql_sc = SQLContext(sc)\n",
    "\n",
    "# Load and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=pd.read_csv(r\"C:\\Users\\yvonn\\OneDrive\\Documents\\WeChat Files\\yifan729796133\\Files\\writeNewDataLog_2018-03-16\\writeNewDataLog_2018-03-16.csv\",sep='\\t',encoding=\"utf-8\")\n",
    "#data1=pd.read_csv(r\"C:\\Users\\yvonn\\OneDrive\\Documents\\WeChat Files\\yifan729796133\\Files\\writeNewDataLog20180315\\writeNewDataLog20180315.csv\",sep='\\t',encoding=\"utf-8\")\n",
    "#data1=pd.read_csv(r\"\\data\\data\\writeNewDataLog_2018-03-19.csv\",sep='\\t',encoding=\"utf-8\",usecols=['user_id_x','user_work_id','activity','score'])\n",
    "#data1 = sql_sc.createDataFrame(data1)\n",
    "\n",
    "data21=pd.read_csv(\"/data/data/writeNewDataLog_2018-03-27.csv\",sep='\\t',encoding=\"utf-8\",usecols=['user_id_x','user_work_id','activity'])\n",
    "data2 = sql_sc.createDataFrame(data21)\n",
    "data31=pd.read_csv(\"/data/data/writeNewDataLog_2018-03-28.csv\",sep='\\t',encoding=\"utf-8\",usecols=['user_id_x','user_work_id','activity','score'])\n",
    "data3 = sql_sc.createDataFrame(data31)\n",
    "data41=pd.read_csv(\"/data/data/writeNewDataLog_2018-03-29.csv\",sep='\\t',encoding=\"utf-8\",usecols=['user_id_x','user_work_id','activity','score'])\n",
    "data4 = sql_sc.createDataFrame(data41)\n",
    "#data5=pd.read_csv(\"/data/data/writeNewDataLog_2018-03-30.csv\",sep='\\t',encoding=\"utf-8\",usecols=['user_id_x','user_work_id','activity','score'])\n",
    "#data5 = sql_sc.createDataFrame(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "temp=data2.union(data3).union(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id_x: long (nullable = true)\n",
      " |-- user_work_id: long (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print out the columns\n",
    "temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42635"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get distinct user_id and user_work_id\n",
    "temp.select('user_id_x').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp.select('user_work_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dataframe to a table\n",
    "temp.registerTempTable('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#use sql command to deal with table defined\n",
    "sqlContext = SQLContext(sc)\n",
    "temp=sqlContext.sql(\"select *,(case when activity = '送礼物' then 12 when activity='评论' then 10 when activity='播放' then 2 when activity='下载' then 6 end) as score from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.dropTempTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "|user_id_x|user_work_id|sum(score)|\n",
      "+---------+------------+----------+\n",
      "|295126169|   319859067|       638|\n",
      "|292972921|   322883555|       314|\n",
      "|288151290|   322063288|       502|\n",
      "| 81112189|   317886919|       102|\n",
      "|297355795|   322883561|       306|\n",
      "+---------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=temp.groupby(['user_id_x','user_work_id']).agg({'score': 'sum'})\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed(\"sum(score)\", \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----+\n",
      "|user_id_x|user_work_id|score|\n",
      "+---------+------------+-----+\n",
      "|295126169|   319859067|  638|\n",
      "|292972921|   322883555|  314|\n",
      "|288151290|   322063288|  502|\n",
      "| 81112189|   317886919|  102|\n",
      "|297355795|   322883561|  306|\n",
      "+---------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('data')\n",
    "sqlContext.sql(\"select user_id_x,user_work_id,score from data\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "user=data.groupby('user_id_x').count().withColumnRenamed(\"count\", \"n\").filter(\"n >= 10\").select('user_id_x')\n",
    "data1=data.join(user,['user_id_x'])\n",
    "work=data1.groupby('user_work_id').count().withColumnRenamed(\"count\", \"n\").filter(\"n >= 10\").select('user_work_id')\n",
    "data2=data1.join(work,['user_work_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|user_work_id|user_id_x|score|\n",
      "+------------+---------+-----+\n",
      "|   115833693| 83279549|  104|\n",
      "|   115833693|287081204|  130|\n",
      "|   115833693|296585419|  102|\n",
      "|   115833693| 52682528|  102|\n",
      "|   115833693| 81894225|  102|\n",
      "+------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.columns=['user_id', 'item_id', 'rating']\n",
    "df=data2\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "ranked =  df.withColumn(\"rank\", dense_rank().over(Window.orderBy(\"user_id_x\")))\n",
    "ranked1=ranked.withColumn(\"rank1\",dense_rank().over(Window.orderBy(\"user_work_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ranked1.select('rank','rank1','score').withColumnRenamed(\"rank\", \"user_id_x\").withColumnRenamed(\"rank1\",\"user_work_id\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7111"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked.select('rank').rdd.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----+\n",
      "|user_id_x|user_work_id|score|\n",
      "+---------+------------+-----+\n",
      "|      156|           1|    2|\n",
      "|      978|           1|  202|\n",
      "|     1276|           1|  202|\n",
      "|     1288|           1|  202|\n",
      "|     2583|           1|    4|\n",
      "+---------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_RDD, test_RDD = df.randomSplit([0.8, 0.2])\n",
    "test_for_predict_RDD = test_RDD.rdd.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.312046139671208\n",
      "5.586589320360797\n",
      "4.716263686889607\n",
      "9.090257774314388\n",
      "5.686783011160867\n",
      "4.680984604051102\n",
      "9.804655026239024\n",
      "6.814099204499685\n",
      "5.655776931261197\n",
      "1.3957310421961147\n",
      "0.8669901762587191\n",
      "0.7021991828106324\n",
      "1.162971409368757\n",
      "0.61751725493901\n",
      "0.5099146979688567\n",
      "0.6810634872258862\n",
      "0.42137206209163103\n",
      "0.35511065921144364\n",
      "0.7072047622923782\n",
      "0.562969783009437\n",
      "0.49071474293718154\n",
      "0.503201449540762\n",
      "0.23130795758928244\n",
      "0.2189607459858207\n",
      "0.21012398673822666\n",
      "0.1532530377693966\n",
      "0.14479691377132708\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "from  pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools,math\n",
    "test_for_predict_RDD = df.rdd.map(lambda x: (x[0], x[1]))\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "#rdd1 = sc.parallelize(ratings)\n",
    "#rdd = sql_sc.createDataFrame(ratings)\n",
    "#testrdd = sc.parallelize(test)\n",
    "#rdd2 = rdd1.map(lambda x: [int(i) for i in x])\n",
    "#df = rdd2.toDF()\n",
    "ranks=[20,60,100]\n",
    "lambdas=[0.2,0.02,0.002]\n",
    "iterations=[5,8,10]\n",
    "for rank, lmbda, numIter in itertools.product(ranks,\n",
    "                                              lambdas,\n",
    "                                              iterations):\n",
    "    model = ALS.train(df, rank, numIter, lmbda)\n",
    "    predictions=model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = df.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473.9801713954781\n",
      "473.27342459816373\n",
      "469.3410918172375\n",
      "482.2407211085714\n",
      "475.66974162095113\n",
      "465.68261119342935\n",
      "490.5522598887366\n",
      "473.65153493334833\n",
      "464.56627664362355\n",
      "472.73578003936194\n",
      "471.7094698739519\n",
      "465.77249160787693\n",
      "474.9631887239488\n",
      "463.5805762346791\n",
      "467.16654834640286\n",
      "455.6525955673623\n",
      "473.57775829891835\n",
      "469.13419642400066\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "from  pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools,math\n",
    "test_for_predict_RDD = test_RDD.rdd.map(lambda x: (x[0], x[1]))\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "#rdd1 = sc.parallelize(ratings)\n",
    "#rdd = sql_sc.createDataFrame(ratings)\n",
    "#testrdd = sc.parallelize(test)\n",
    "#rdd2 = rdd1.map(lambda x: [int(i) for i in x])\n",
    "#df = rdd2.toDF()\n",
    "ranks=[100,200]\n",
    "lambdas=[0.2,0.02,0.002]\n",
    "iterations=[5,8,10]\n",
    "for rank, lmbda, numIter in itertools.product(ranks,\n",
    "                                              lambdas,\n",
    "                                              iterations):\n",
    "    model = ALS.train(training_RDD, rank, numIter, lmbda)\n",
    "    predictions=model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = test_RDD.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
